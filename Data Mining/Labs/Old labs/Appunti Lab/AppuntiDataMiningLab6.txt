Carichiamo una nuova libreria, che si chiama `wine`.
```
data(wine, package='rattle')
install.packages('rattle')
```
Qui vengono contenuti dati sulle varie tipologie di vino, ovvero
```
wine[1,]
```
Mentre il tipo di vino lo possiamo vedere qui:
```
table(wine$Type)
```

Creiamo un grafico a barre:
```
barplot(table(wine$Type))
```

Ora vediamo una soluzione grafica per la distribuzione dei vini:
```
pairs(wine[,2:7])
```

Vediamo anche:
```
pairs(wine[,8:13])
```

Facciamo una analisi discriminante lineare ora:
```
library(MASS)
wine.lda <- lda(Type ~ ., data=wine)
```

Dando wine.lda vediamo il seguente output:
```
> wine.lda
Call:
lda(Type ~ ., data = wine)

Prior probabilities of groups:
        1         2         3 
0.3314607 0.3988764 0.2696629 

Group means:
   Alcohol    Malic      Ash Alcalinity Magnesium  Phenols Flavanoids
1 13.74475 2.010678 2.455593   17.03729  106.3390 2.840169  2.9823729
2 12.27873 1.932676 2.244789   20.23803   94.5493 2.258873  2.0808451
3 13.15375 3.333750 2.437083   21.41667   99.3125 1.678750  0.7814583
  Nonflavanoids Proanthocyanins    Color       Hue Dilution   Proline
1      0.290000        1.899322 5.528305 1.0620339 3.157797 1115.7119
2      0.363662        1.630282 3.086620 1.0562817 2.785352  519.5070
3      0.447500        1.153542 7.396250 0.6827083 1.683542  629.8958

Coefficients of linear discriminants:
                         LD1           LD2
Alcohol         -0.403399781  0.8717930699
Malic            0.165254596  0.3053797325
Ash             -0.369075256  2.3458497486
Alcalinity       0.154797889 -0.1463807654
Magnesium       -0.002163496 -0.0004627565
Phenols          0.618052068 -0.0322128171
Flavanoids      -1.661191235 -0.4919980543
Nonflavanoids   -1.495818440 -1.6309537953
Proanthocyanins  0.134092628 -0.3070875776
Color            0.355055710  0.2532306865
Hue             -0.818036073 -1.5156344987
Dilution        -1.157559376  0.0511839665
Proline         -0.002691206  0.0028529846

Proportion of trace:
   LD1    LD2 
0.6875 0.3125 
```

Dove:
- Group means: indica la media delle variabili
- Coefficients of linear discriminants: discriminanti
- Proportion of trace: Percentuale di separazione tra gruppi

Se plottiamo il modello possiamo vedere come la separazione tra i gruppi sara' evidente.
```
plot(wine.lda)
```

Questo ci da un grafico diverso rispetto alle altra volte, in quanto abbiamo piu' funzioni discriminanti. In questo diagramma di dispersione vengono riportate le funzioni discriminanti in ascisse e in ordinata, e questo ci fa vedere la capacita' delle funzioni discriminanti, che vengono segnate con 1, 2, 3. Possiamo vedere che 1 e 2 sono abbastanza separati, mentre 2 e 3 sono piu' separati rispetto a 1 e 2. Quindi c'e' una soddisfancente separazione, confermata dal fatto che la separazione ci diceva che e' del 69% (0.6875).
LD2 invece e' basso perche' 1 copre 3 in Y. LD2 separa bene solamente 1 e 2, ed infatti otteniamo una percentuale bassa (31%).

Eseguiamo una previsione ora:
```
wine.previsioni <- predict(wine.lda)
```

La funzione `ldahist` ci riporta l'istogramma della discriminante lineare. Diamo da R il seguente comando:
```
ldahist(wine.previsioni$x[,1], g=wine$Type)
```

Costruiamo l'istrogramma in questo modo:
- Prima funzione della discriminante lineare (LD1)
- Rappresentiamo gli istogrammi secondo il tipo di vino

Quello che otteniamo sono quindi le funzioni discriminanti rappresentate graficamente. I grafici confermano quanto avevamo detto a parole.
Eseguiamo la stessa operazione per LD2:
```
ldahist(wine.previsioni$x[,2], g=wine$Type)
```
Anche questo grafico conferma i nostri sospetti su come 1 e 3 si sovrappongano tra loro.
Ripuliamo la console in quanto ora cambieremo dataset, riprendendo quello Boston:
```
rm(list=ls())
data(Boston)
dim(Boston)
```
Rivediamo le variabili:
```
names(Boston)
```

Ricreiamo il modello iniziale, ma aggiungiamo anche piu' frafi
```
m <- lm(medv ~ lstat, data=Boston)
m2 <- lm(medv ~ lstat + I(lstat^2), data=Boston)
m3 <- lm(medv ~ lstat + I(lstat^2) + I(lstat^3), data=Boston)
```

Vedendo il summary di m3, solo i p-value mi direbbe che e' un modello da tenere.
Ora vediamo l'AIC:
```
aic.m2 <- 2*2 - 2*logLik(m2)
aic.m2
```
Usiamo `logLik`
```
logLik(m2)
```

Facciamo la stessa cosa anche per m3 ora:
```
aic.m3 <- 2*2 - 2*logLik(m3)
aic.m3
```
Questo ci dice che il modello migliore e' m3, perche' *migliore e' AIC migliore e' il modello*. Se la differenza e' maggiore di 1 allora significa che c'e' differenza tra i due modelli e quindi vale la pena cambiare il modello, altrimenti se e' compreso in 1 significa che i due modelli non variano significativamente.
Eseguendo questo conto otteniamo:
```
3166.516-3143.796
```
Vediamo come la differenza sia altissimo.
Ora vediamo il modello generalizzato per m2.
```
m2.glm <- glm(medv ~ lstat + I(lstat^2), data=Boston)
```
Dando il summary, l'output ci dice che e' un modello lineare in quanto ci viene detto che viene usata una gaussiana. Modifichiamo anche m3:
```
m3.glm <- glm(medv ~ lstat + I(lstat^2) + I(lstat^3), data=Boston)
```
Ora eseguiamo la differenza tra i due AIC con questo tipo di modello:
```
m2.glm$aic - m3.glm$aic
```
Anche questa volta si ottiene 22.72039, che ci dice che e' meglio tenere m3. Possiamo automatizzare questa procedura (che altrimenti a mano sarebbe lentissimo) srivendo un programmino.
Per calcolare la BIC dovremo dare: `2*log(n) - 2*log-verosomiglianza`.
Facendo i calcoli otterremmo un AIC che ci dice di usare un polinomio di ordine 5, mentre con l'R2 aggiustato ci verrebbe consigliato di usare un polinomio di ordine 6.

Ora carichiamo una libreria aggiuntiva per eseguire la cross-validation:
```
library(boot)
```
Creiamo ora il modello con polinomio di grado 5 e 6:
```
m5.glm <- glm(medv ~ poly(lstat, 5), data=Boston)
m6.glm <- glm(medv ~ poly(lstat, 6), data=Boston)
```

Calcoliamo ora il CV 10-folds per m5
```
cv.m5 <- cv.glm(Boston, m5.glm, K=10)
```
Vediamo cos'e' presente dentro cv:
```
names(cv.m5)
```
Vediamo i ldelta:
```
cv.m5$delta
```
Facciamolo anche per il modello di grado 6:
```
cv.m6 <- cv.glm(Boston, m6.glm, K=10)
```
E il delta:
```
cv.m6$delta
```
m6 e' leggermente piu' grande, quindi si potrebbe tenere quella di grado 5.
Calcoliamo il LOOCV:
```
loocv.m5 <- cv.glm(Boston, m5.glm)
loocv.m6 <- cv.glm(Boston, m6.glm)
```
Vediamo i delta:
```
loocv.m5$delta
loocv.m6$delta
```
Siccome la differenza in questo caso e' pochissima ci teniamo con il modello di grado 5, non vale la pena passare a quello di grado 6.

Carichiamo un altro dataset:
```
library(ISLR)
data(Hitters)
dim(Hitters)
names(Hitters)
```

Questo modello sono i giocatori di Baseball. In questo modello potrebbero mancare dei dati, in quanto dipende dalla natura dei dati stessi.
Quindi come ci si muove in questo caso? In queste situazioni in cui manca un dato in una colonna di una riga si butta via tutta la riga.
R ci permette di farlo automaticamente, usando il comando `na.omit` che omette i dati incompleti:
```
hitters <- na.omit(Hitters)
dim(hitters)
```
Vediamo come sono stati eliminati 59 righe.
Abbiamo:
```
263/20
```
Che ci da 13.15, dicendoci quindi che abbiamo pochi dati.
Carichiamo una nuova libreria adesso:
```
library('leaps')
```
Usiamo la funzione `regsubset`, che prende variabili esplicative (8 di default).
```
m.forward <- regsubsets(Salary ~ ., data=hitters, nvmax=19, method='forward')
```
Questo ci dice se abbiamo obbligato qualche variabile o a non entrare mai o a essere sempre presente nel modello. Contiene una tabella con variabile true o false.
Possiamo anche stampare il summary, dando `summary(m.forward)`.
Il `names` del modello ci dice cos'e' contenuto nel modello:
```
names(summary(m.forward))
```

In questo caso ci interessa a noi:
- rsq
- rss
- adjr2

Possiamo vedere come manca l'AIC, ma e' presente il CP.
Per ottenere l'RSS piu' piccolo dobbiamo fare:
```
min(summary(m.forward)$rss)
```
Il prossimo comando ci da la posizione di quello con Rss piu' basso.
```
which.min(summary(m.forward)$rss)
```
Facciamo la stessa cosa per il BIC ora.
```
which.min(summary(m.forward)$bic)
```
La riduzione che otteniamo e' da 19 a 6. Questo e' importante, e prendiamo quello con il BIC minore.
```
coef(m.forward,6)
```
Con il comando vcov otteniamo la matrice con tutte le varianze e le covarianze della stima tra coefficenti:
```
vcov(m.forward, 6)
```
Con `diag(vcov(m.forward, 6))` ci da i;'intervallo di confidenza delle variabili. Ora possiamo eseguire anche un plot del modello:
```
plot(m.forward)
```

Vediamo le altre selezioni, mettendolo in un grafico 2x2:
```
par(mfrow=c(2,2))
```
Eseguiamo il plot adesso:
```
plot(m.forward)
plot(m.forward, scale='adjr2')
plot(m.forward, scale='r2')
```

Verifichiamo ora i coefficenti:
```
coef(m.forward, 6)
```
Stimiamo un nuovo modello lineare:
```
m <- lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data=hitters)
```
Vediamo il grafico dei residui:
```
par(mfrow=c(2,2))
plot(m)
```
