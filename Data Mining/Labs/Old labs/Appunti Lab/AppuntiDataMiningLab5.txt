04/05/2017
Cominciamo con una dataset diverso oggi, chiamato mtcars

data(mtcars)

Questo dataset e' piccolino ( `dim(mtcars)` ) e presenta le performance di modelli automobilistici dal 73 al 74 (vedesi `?mtcars`)
Analizzeremo le variabili:
- V/S (qualitativa) = 0, conf. V
                      1, inlinea
- mpg (quantitativa)= consumo gallone per miglio
- am (qualitativa)  = 0, no cambio automatico
                      1, si cambio automatico

Costruiamo un dataset piccolo con le variabili che ci interessano, che possiamo vedere con `mtcars[1,]`.
Quindi diamo:
```
dati <- mtcars[, c('mpg', 'vs', 'am')]
```
La dimensione dei dati ora e':
```
dim(dati)
```
Vediamo meglio con:
```
dati[1:4,]
```
Verifichiamo che tipo di variabile sia am, con il comando:
```
is.factor(dati$am)
```
Per vedere se e' numerica:
```
is.numeric(dati$am)
```
Quindi R intende questa variabile come _quantitativa_, e dobbiamo tradurla in _qualitativa_. Per farlo, diamo il comando:
```
dati$am <- as.factor(dati$am)
```
E ora dando di nuovo `is.factor(dati$am)` otterremo `TRUE`.
Ora creiamo un boxplot come segue:
```
boxplot(dati$mpg ~ dati$vs)
```

Questo grafico ci dice che mpg ~ vs influenzano molto il modello in quanto le due scatole sono molto distanti tra loro

Ora vediamo con piu' dati:
```
boxplot(dati$mpg ~ dati$vs * dati$am)
```

In questa maniera abbiamo per esempio:
1.0 = vm 1 am 0.
Qui abbiamo che vm influenza la risposta, ma ha senso una interazione tra am e mpg? Dipende. Non c'e' una separazione chiarissima tra boxplot, ma sembra che comunque am influenzi le variazioni di vs. Quindi vedremo meglio con il modello se questa interazione andra' bene o no.
Creiamo quindi il nostro modello generalizzato lineare:
```
modello <- glm(vs ~ mpg * am, data=dati, family=binomial)
```

E' importante specificare .. altrimenti R creera' un modello lineare "classico". E' necessario specificare quindi la famiglia a cui appartiene il modello.
Di default quindi R applichera' il modello con la seguente funzione di logit:

logit: P(vs)
       -----
       1-P(vs)

Vediamo il summary:
```
summary(modello)
```
Otteniamo il seguente output:
```
> summary(modello)

Call:
glm(formula = vs ~ mpg * am, family = binomial, data = dati)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.70566  -0.31124  -0.04817   0.28038   1.55603  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept) -20.4784    10.5525  -1.941   0.0523 .
mpg           1.1084     0.5770   1.921   0.0547 .
am1          10.1055    11.9104   0.848   0.3962  
mpg:am1      -0.6637     0.6242  -1.063   0.2877  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 43.860  on 31  degrees of freedom
Residual deviance: 19.125  on 28  degrees of freedom
AIC: 27.125

Number of Fisher Scoring iterations: 7
```

Significato:
- La frase `(Dispersion parameter for binomial family taken to be 1)` significa che a volte i dati potrebbero essere troppo variabili, e questo parametro ci aiuta a regolarli. Per questo tipo di modello sara' sempre 1.
- Devianza e': 2(L_max - L_corrente). Ricordiamo che DEV ~ X^2_n-p-1. Piu' la devianza e' piccola piu' il modello va bene. Si rifiuta il modello corrente per valori grandi della devianza.:
  + La devianza nulla ci da la distanza del modello da quello in cui e' presente solo l'intercetta. In questo caso la riga `Null deviance: 43.860  on 31  degrees of freedom` ci dice che ovviamente va male. In questo caso la devianza e' troppo alta e il modello senza variabili va male. Se ci calcoliamo il quantile con `qchisq(0.95, 31)` ci da 44.98534, che ci fa dire che siamo nella soglia del rifiuto. Questo non ci sorprende, perche' a parte `mpg`, tutto il resto sarebbe da buttare via.
  + La devianza residua confronta la verosomiglianza massima con il modello corrente. In questo caso e' `Residual deviance: 19.125  on 28  degrees of freedom`. Potremmo applicare lo stesso principio di prima, ma calcoliamo l'alpha osservato prima. L'aplha osservato sarebbe coerente, perche' sarebbe la probabilita' di vedere qualcosa piu' estremo di 19.125. Troviamo il quantile facendo 1-pchisq(19.125, 28), che vale 0.8942349. Questo ci da un livello di significativita' altissimo, e ci dice che e' inutile aggiuntere tutte quelle variabili al modello. Quindi il modello scritto cosi' ci dice che non e' significativo.
- `Number of Fisher Scoring iterations: 7` E' il numero di iterazioni per raggiungere la convergenza. E' meglio che ci siano valori bassi di questo, e valori alti indicano che i dati sono sparsi.

Quindi siamo autorizzati a semplificare il modello. Rimuoviamo quindi prima l'interazione `mpg:am1`.
Togliamo l'interazione con il seguente modello quindi:
```
modello2 <- update(modello, . ~ . - mpg:am)
```
Vediamo il summary:
```
> summary(modello2)

Call:
glm(formula = vs ~ mpg + am, family = binomial, data = dati)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.05888  -0.44544  -0.08765   0.33335   1.68405  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)   
(Intercept) -12.7051     4.6252  -2.747  0.00602 **
mpg           0.6809     0.2524   2.698  0.00697 **
am1          -3.0073     1.5995  -1.880  0.06009 . 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 43.860  on 31  degrees of freedom
Residual deviance: 20.646  on 29  degrees of freedom
AIC: 26.646

Number of Fisher Scoring iterations: 6

```
Consideriamo sul limite am1, con il suo z-value di 0.06009.
Con la funzione `anova` confrontiamo i due modelli. Confrontiamoli scrivendo:
```
anova(modello2, modello)
```
La funzione anova capisce che analisi deve eseguire. Se vogliamo anche la significativita' del modello aggiungiamo l'opzione `test='Chisq'`. Quindi:
```
anova(modello2, modello, test='Chisq')
```
L'outuput ci di dice i valori della devianza (20.646 e 19.125), ed esegue i seguenti test:
- M2 ⊃ M1
  p=3  p=2
- D2 ≤ D1
  D1 - D2 ~ X^2_3-2, che ci da 1.5214
  Questo ci dice che passiamo al modello piu' piccolo, in quanto `qchisq(0,95, 1) e' di 3.841459.
  L'alpha osservato e' `1-pchisq(1.5214, 1)` = 0.2174078 che ci conferma ancora che possiamo passare al modello piu' piccolo.

Se vogliamo vedere i coefficenti diamo:
```
coef(modello2)
```
Vediamo l'intervallo di confidenza:
```
0.6809 - qnorm(0.95) * 0.2524

0.6809 + qnorm(0.95) * 0.2524
```
La verifica di ipotesi e':
```
confint(modello2)
```

Proviamo a vedere se si puo' semplificare ancora di piu' il modello togliendo ami1 con:
```
modello3 <- update(modello2, . ~ . - am)
```

Il summary ci da:
```
> summary(modello3)

Call:
glm(formula = vs ~ mpg, family = binomial, data = dati)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2127  -0.5121  -0.2276   0.6402   1.6980  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)   
(Intercept)  -8.8331     3.1623  -2.793  0.00522 **
mpg           0.4304     0.1584   2.717  0.00659 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 43.860  on 31  degrees of freedom
Residual deviance: 25.533  on 30  degrees of freedom
AIC: 29.533

Number of Fisher Scoring iterations: 6

```
Confrontiamo con il modello precedenti:
```
anova(modello3, modello2)
```
Che ci dice che ci teniamo `modello2`, nonostante `am1` abbia z-value alto.
Verifichiamo i grafici dei residui con:
```
par(mfrow=c(2,2))
plot(modello2)
```
I grafici che abbiamo vanno bene. I grafici 1 e 3 non si possono leggere in quanto (?). Nel grafico 2 abiamo pochi punti e siamo confrontando una variabile quantitativa con una qualitativa. Stessa cosa per 4. Per questo l'analisi dei residui non si fa, e si fa l'analisi della devianza.

Come si stimano i valori del modello?
Per stimare il modello diamo:
```
valori.stimati <- predict(modello2)
```
Questi non ci ritornano probabilita', perche' vanno da -inf a +inf.
Se vogliamo le probabilita' invece diamo:
```
prob.stimate <- predict(modello2, type='response')
```
Possiamo rappresentare la prob.stimate per valore di am (quindi per cambio automatico o no). Per fare cio' prima rappresentiamo questi dati:
```
plot(dati$mpg, dati$vs)
```
Sono dati 0/1, quindi va bene che siamo su due linee.
Ora dobbiamo rappresentare `P(vs=1) = e^{beta0 + beta1 mpg + beta2 am}
                                      --------------------------------
                                      1 + e^{beta0 + beta1 mpg + beta2 am}
che e' l'inverso del logit.
Per plottare cio' usiamo il comando `curve()`. Dobbiamo specificare solo le `x`, le `y` se le prende dal grafico esistente per magia.
```
curve(predict(modello2, newdata=data.frame(mpg=x, am='0'), type='response'), add=TRUE)
```
Il comando `curve` ha bisogno solo dell'espressione per costruire la curva. In automatico lui prende le ascisse come quelle che gli specificate come `x`.
Diamo lo stesso comando ora per `am=1`. Cambiamo un attimo anche il layout.
```
curve(predict(modello2, newdata=data.frame(mpg=x, am='1'), type='response'), add=TRUE, lty=2, lwd=2)
```
Le curve sono "parallele", procedono in modo da non avere in croci quando fanno le curve, altrimenti ci sarebbe interazione.
Verifichiamo le previsioni
```
previsioni <- rep(0, nrow(dati))
```
Questo ci ha creato un array con 32 zeri.
Ora diamo:
```
previsioni[ prob.stimate > 0.5 ] <- 1
```
Che inserisce 1 quando le prob.stimate sono > 0.5.

Per costruire una tabella di classificazione ora diamo:
```
table(previsioni, dati$vs)
```
La tabella ci dice che 15 hanno valore 0, e 10 sono classificate come 1.
Il modello ha quindi un errore di 7/32 = 0.21875, che chiamiamo come il _training error rate_.
Di solito l'errore nel training set e' sempre piu' basso di quello che si fa nel test set. Quindi consideriamo questo errore come "alto".
Come facciamo a verificare i dati fuori dal test set? Per vedere se il nostro modello va bene anche fuori dai nostri dati bisogna in qualche modo sfruttarli diversamente. Un'idea molto semplice e': prendo tutti i dati (100%), li spezziamo in due gruppi in modo casuale (dividiamo in due gruppi). Mettamo caso che venga 60%/40%. Stimeremo il modello2 dentro il 60%, e useremo il modello di questo ridotto training set per vedere la previsione dentro il 40%, che si viene definito come un fittizio test set (sottoinsieme di previsione).
Le procedure automatiche di valutazione del modello fanno questa cosa a piu' riprese prendendo una suddivisione ottimale del modello. Facciamolo a mano per vedere:
```
n <- nrow(dati)
```
Selezioniamo il seed a un valore fissato:
```
set.seed(222)
```
E prendiamo un campione con il comando `sample`, in cui specifichiamo la percentuale dei dati che vogliamo che vengano presi.
```
selezione <- sample(n, 0.60*n, replace=FALSE)
```
Questi li definiamo come il training set dei dati:
```
training.set <- dati[selezione,]
```
Definiamo il test set come:
```
test.set <- dati[-selezione,]
```
Ricreamo il modello con i dati del training set:
```
modello2.training <- glm(vs ~ mpg + am, data=training.set, family=binomial)
```
Vediamo il summary che ci ritorna:
```
> summary(modello2.training)

Call:
glm(formula = vs ~ mpg + am, family = binomial, data = training.set)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.80226  -0.17226  -0.00836   0.10825   1.37418  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept) -24.0502    15.2087  -1.581   0.1138  
mpg           1.3258     0.8175   1.622   0.1048  
am1          -5.4999     3.2141  -1.711   0.0871 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 26.2869  on 18  degrees of freedom
Residual deviance:  7.2473  on 16  degrees of freedom
AIC: 13.247

Number of Fisher Scoring iterations: 8
```
Non ci deve spaventare il fatto che ora il modello non vada bene.
Ora diamo:
```
prob.test <- predict(modello2.training, newdata=test.set, type='response')
```
Ricostruiamoci le previsioni con:
```
previsioni.test <- rep(0, nrow(test.set))
previsioni.test[prob.test>0.5] <- 1
```
Creiamo la tabella delle previsioni con:
```
table(previsioni.test, test.set$vs)
```
Che ci da una percentuale di errore di 4/13 = 0.3076923


Vediamo un'altra cosa. Carichiamo la libreria MASS:
```
library(MASS)
```
Creiamo il modello:
```
modello.lda <- lda (vs ~ mpg + am, data=training.set)
```
Che ci da:
```
> modello.lda
Call:
lda(vs ~ mpg + am, data = training.set)

Prior probabilities of groups:
        0         1 
0.5263158 0.4736842 

Group means:
       mpg       am1
0 16.53000 0.5000000
1 23.95556 0.4444444

Coefficients of linear discriminants:
           LD1
mpg  0.3263393
am1 -1.9399213
```
La funzione distriminante e' del tipo: 0.326 * mpg - 1.9399 * am1
L'unica cosa che puo' risultare un po' strana e' che i risultati potrebbero non essere esattamente questi, e' che R regolarizza i dati in modo da riscalarli, quindi usa la stessa funzione ma su dati riscalati.
Analizziamo il grafico:
```
plot(modello.lda)
```
Questo e' l'istrogramma della classificazione a gruppi. Viene assegnato al gruppo 0 una serie di assegnazioni. A livello numerico non e' possibile valutare il modello, ma solo graficamente. In questo caso va male, perche' e' presente una certa sovrapposizione. Il modello quindi _non_ fa un'ottima classificazione. Perche' fosse perfetta i gruppi dovrebbero essere separati.

Le previsioni di questo modello si ottengono solamente con il comando `predict` senza alcun parametro:
```
previsioni.lda <- predict(modello.lda, test.set)
```
Visualizziamo l'output:
```
> previsioni.lda
$class
 [1] 1 1 0 1 0 0 0 1 1 0 0 1 0
Levels: 0 1

$posterior
                               0           1
Hornet Sportabout   0.2811119934 0.718888007
Valiant             0.3909385286 0.609061471
Duster 360          0.9367503419 0.063249658
Merc 240D           0.0035156943 0.996484306
Merc 450SE          0.7232788641 0.276721136
Merc 450SL          0.5541392254 0.445860775
Lincoln Continental 0.9973129490 0.002687051
Honda Civic         0.0033586740 0.996641326
Toyota Corolla      0.0001870872 0.999812913
Dodge Challenger    0.8460772662 0.153922734
AMC Javelin         0.8756593016 0.124340698
Porsche 914-2       0.1131897759 0.886810224
Volvo 142E          0.8508024734 0.149197527

$x
                            LD1
Hornet Sportabout    0.47921081
Valiant              0.28340723
Duster 360          -0.95668212
Merc 240D            2.33934484
Merc 450SE          -0.27136959
Merc 450SL           0.02233579
Lincoln Continental -2.22940541
Honda Civic          2.35745937
Toyota Corolla       3.49964693
Dodge Challenger    -0.56507496
AMC Javelin         -0.66297675
Porsche 914-2        0.92156644
Volvo 142E          -0.57959436
```
Facendo la tabella di classificazione di questo modello vediamo:
```
table(previsioni.lda$class, test.set$vs)
```
Vediamo che fa meno errori, solo 3/13 = 0.2307692
Vediamo il grafico della curva ROC. La curva ROC e' presente in diversi pacchetti, carichiamo:
```
install.packages('pROC')
library(pROC)
```
Ora per disegnare la curca ROC diamo:
```
valori.roc <- roc(test.set$vs, previsioni.lda$posterior[,2])
plot(valori.roc)
```
Visualizziamo meglio la nostra curva ROC, con il seguente comando:
```
plot(valori.roc, legacy.axes=TRUE, print.auc=TRUE, auc.polygon=TRUE)
```
