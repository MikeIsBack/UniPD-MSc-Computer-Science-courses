Carichiamo il dataset su auto (molto piccolo, due variabili, serve per vedere le spline)
```
data(cars)
dim(cars)
cars[1:4,]
```

Dove nel grafico la y e' la distanza dist, e la x e' la velocita' (speed)
Plottiamo il grafico
```
plot(cars$speed, cars$dist)
```

Ci potrebbe passare una retta, che coglierebbe la maggiorparte dei punti perdendosi pero' tutti i punti che sono verso l'alto. Questi punti non son tanti ma siccome abbiamo 50 osservazioni pesano lo stesso. Anche altri polinomi piu' grandi, come la parabola, ci starebbe, il problema e' che la variabilita' delle osservazioni cambia, e quindi una osservazione non lineare dell'andamento coglierebbe meglio l'andamento.
Creiamo un modello lineare per il momento:
```
m.lm <- lm(dist ~ speed, data=cars)
> summary(m.lm)

Call:
lm(formula = dist ~ speed, data = cars)

Residuals:
    Min      1Q  Median      3Q     Max 
-29.069  -9.525  -2.272   9.215  43.201 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 15.38 on 48 degrees of freedom
Multiple R-squared:  0.6511,	Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
```

Potremmo provare un polinomio di qualsiasi ordine. Proviamo do ordine 3.
```
m.poly <- lm(dist ~ poly(speed, 3), data=cars)
> summary(m.poly)

Call:
lm(formula = dist ~ poly(speed, 3), data = cars)

Residuals:
    Min      1Q  Median      3Q     Max 
-26.670  -9.601  -2.231   7.075  44.691 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)        42.98       2.15  19.988  < 2e-16 ***
poly(speed, 3)1   145.55      15.21   9.573  1.6e-12 ***
poly(speed, 3)2    23.00      15.21   1.512    0.137    
poly(speed, 3)3    13.80      15.21   0.907    0.369    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 15.2 on 46 degrees of freedom
Multiple R-squared:  0.6732,	Adjusted R-squared:  0.6519 
F-statistic: 31.58 on 3 and 46 DF,  p-value: 3.074e-11
```

Il grado 2 e il grado 3 non colgono tutta la variabilita' che abbiamo visto nel grafico.
Cominciamo quindi a costruire una spline. Costruiamo una spline di regressione.
Costruiamo direttamente un modello con una spline naturale, di ordine 3.
```
m.ns <- lm(dist ~ ns(speed, 3), data=cars)
```

Se non funziona il comando, c'e' questa libreria da installare:
```
library(splines)
```

Vediamo il summary:
```
> summary(m.ns)

Call:
lm(formula = dist ~ ns(speed, 3), data = cars)

Residuals:
    Min      1Q  Median      3Q     Max 
-26.789  -9.750  -2.421   7.326  44.203 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)      2.594      9.057   0.286 0.775875    
ns(speed, 3)1   35.959      8.731   4.118 0.000157 ***
ns(speed, 3)2   96.444     20.561   4.691 2.46e-05 ***
ns(speed, 3)3   72.986      8.021   9.099 7.49e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 15.23 on 46 degrees of freedom
Multiple R-squared:  0.6722,	Adjusted R-squared:  0.6508 
F-statistic: 31.44 on 3 and 46 DF,  p-value: 3.298e-11
```

Spline naturale fino di ordine 3 utile e ha un p-value basso. Non e' possibile ridurre ulteriolmente.
Proviamo ora a creare una smoothing spline.
Diamo
```
set.seed(2906)
m.sp.cv <- smooth.spline(x=cars$speed, y=cars$dist, cv=TRUE)
```

Questo ci da un warning.
Se vediamo:
```
> names(m.sp.cv)
 [1] "x"        "y"        "w"        "yin"      "data"     "lev"     
 [7] "cv.crit"  "pen.crit" "crit"     "df"       "spar"     "lambda"  
[13] "iparms"   "fit"      "call"    
```
In questo caso, lambda e df sono praticamente la stess cosa, e' solo che il modello e' costruito con lambda che fa come da penalizzazione. Piu' la penalizzazione e' alta meno variabilita' c'e'. Quando cresce lambda diminuisce df.
```
> m.sp.cv
Call:
smooth.spline(x = cars$speed, y = cars$dist, cv = TRUE)

Smoothing Parameter  spar= 1.483488  lambda= 13419.8 (30 iterations)
Equivalent Degrees of Freedom (Df): 2.000009
Penalized Criterion: 4588.73
PRESS: 246.4053
```
Costruiamo il modello ora con df pari a 2 (ovvero con due gradi di liberta')
```
m.sp <- smooth.spline(x=cars$speed, y=cars$dist, df=m.sp.cv$df)
```
Questo ci crea una griglia di valori di velocita' in ascissa.
```
new.speed <- seq(min(cars$speed), max(cars$speed), length.out=100)
```
Avremmo potuto dare anche il seguente commento: `new.speed <- seq(min(cars$speed), max(cars$speed), by= # ad esempio 0.05)`
Il prossimo comando ci da la previsione, in cui gli diamo il modello lineare e il nuovo dataset.
```
lines(new.speed, predict(m.lm, newdata=data.frame(speed=new.speed)), lty=1)
```
Ora lo rifacciamo con il modello polinomiale:
```
lines(new.speed, predict(m.poly, newdata=data.frame(speed=new.speed)), lty=2)
```
possiamo notare come non vada benissimo.
Ora con la natural spline:
```
lines(new.speed, predict(m.ns, newdata=data.frame(speed=new.speed)), lty=3)
```
Ora disegniamo con la smoothing spline:
```
lines(m.sp, lty=4, lwd=2)
```
NOTA: qui il comando e' leggermente diverso.
######

Carichiamo un'altra libreria
```
library(ISLR)
data(College)
dim(College)
names(College)
```
In questo caso vedremo come sara' piu' utile lavorare in scala logaritmica con log(y).
Cambiamo direttamenten nel modello:
```
College$Apps <- log(College$Apps)
```
Selezioniamo le variabili Apps, Private, PhD, S.F.Ratio, Accept.
Costruiamo un insieme di dati che e' un sottoinsieme di college prendendo tutte le righe di College e solo queste cinque colonne:
```
dati <- College[, c('Apps', 'Private', 'PhD', 'S.F.Ratio', 'Accept')]
dim(dati)
```

Facciamo il grafico delle relazioni tra le variabili:
```
pairs(dati[, -2], pch='.')
```

Esistono relazioni che non sono lineari. Proviamo a mettere una spline sulla prima due e una variabile sulla terza. Puo' essere che se tenga valori alti sulla sline nelle prima due puo' essere che poi le variabili vada bene.
Creiamo il modello con la smoothing spline
```
phd.cv <- smooth.spline(x=dati$PhD, y=dati$Apps, cv=TRUE)
> phd.cv
Call:
smooth.spline(x = dati$PhD, y = dati$Apps, cv = TRUE)

Smoothing Parameter  spar= 0.7964489  lambda= 0.020413 (14 iterations)
Equivalent Degrees of Freedom (Df): 5.257
Penalized Criterion: 66.28162
PRESS: 0.8298997
```

Ora facciamo la spline naturale di livello 3.
```
phd.ns3 <- lm(Apps ~ ns(PhD, 3), data=dati)
```
Creiamo quella di livello 4
```
phd.ns4 <- lm(Apps ~ ns(PhD, 4), data=dati)
```
Vediamo l'AIC:
```
> extractAIC(phd.ns3)
[1]    4.0000 -148.3151
> extractAIC(phd.ns4)
[1]    5.0000 -147.5264
```
In questo caso non c'e' il migliore.
Per la smoothing spline _non_ si puo' calcolare l'AIC, in quanto la smoothing spline viene fuori da un processo di cross-validation.

Per PhD smoothing 5, per la spline usiamo quella naturale di livello 3.
SFRation spline naturale 3, smoothing spline 5.

Ora creiamo il GSM di spline naturali:
```
m.gam.ns <- lm(Apps ~ Private + ns(PhD, 3) + ns(S.F.Ratio, 3) + poly(Accept, 2), data=dati)
> summary(m.gam.ns)

Call:
lm(formula = Apps ~ Private + ns(PhD, 3) + ns(S.F.Ratio, 3) + 
    poly(Accept, 2), data = dati)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.35755 -0.24562  0.03378  0.26517  3.07669 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        7.23114    0.21551  33.554  < 2e-16 ***
PrivateYes        -0.01831    0.04994  -0.367   0.7140    
ns(PhD, 3)1        0.62037    0.10084   6.152 1.23e-09 ***
ns(PhD, 3)2        0.95419    0.36224   2.634   0.0086 ** 
ns(PhD, 3)3        0.85483    0.11152   7.665 5.41e-14 ***
ns(S.F.Ratio, 3)1  0.30515    0.12299   2.481   0.0133 *  
ns(S.F.Ratio, 3)2 -0.81222    0.36281  -2.239   0.0255 *  
ns(S.F.Ratio, 3)3 -0.50849    0.36466  -1.394   0.1636    
poly(Accept, 2)1  20.85228    0.57889  36.021  < 2e-16 ***
poly(Accept, 2)2  -9.99342    0.49444 -20.212  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4614 on 767 degrees of freedom
Multiple R-squared:  0.8174,	Adjusted R-squared:  0.8153 
F-statistic: 381.6 on 9 and 767 DF,  p-value: < 2.2e-16
```

Ora vediamo come si stima quando inseriamo tutte smoothing splide
```
library(gam)
```

Ora rimettiamo tutte le componente e mettiamole come smoothing sline dell'ordine visto prima:
```
m.gam.sp <- gam(Apps ~ Private + s(PhD, 5) + s(S.F.Ratio, 5) + poly(Accept, 2), data=dati)
> summary(m.gam.sp)

Call: gam(formula = Apps ~ Private + s(PhD, 5) + s(S.F.Ratio, 5) + 
    poly(Accept, 2), data = dati)
Deviance Residuals:
     Min       1Q   Median       3Q      Max 
-2.31402 -0.24575  0.02991  0.26899  3.07512 

(Dispersion Parameter for gaussian family taken to be 0.2106)

    Null Deviance: 894.5761 on 776 degrees of freedom
Residual Deviance: 160.6569 on 762.9996 degrees of freedom
AIC: 1010.348 

Number of Local Scoring Iterations: 2 

Anova for Parametric Effects
                 Df Sum Sq Mean Sq  F value    Pr(>F)    
Private           1 194.18 194.183 922.2245 < 2.2e-16 ***
s(PhD, 5)         1 172.11 172.106 817.3723 < 2.2e-16 ***
s(S.F.Ratio, 5)   1   1.52   1.523   7.2308  0.007323 ** 
poly(Accept, 2)   2 334.27 167.133 793.7549 < 2.2e-16 ***
Residuals       763 160.66   0.211                       
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Anova for Nonparametric Effects
                Npar Df Npar F     Pr(F)    
(Intercept)                                 
Private                                     
s(PhD, 5)             4 2.9668 0.0189732 *  
s(S.F.Ratio, 5)       4 4.7262 0.0009002 ***
poly(Accept, 2)                             
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

Il college privato viene testato per vedere se rimane dentro o fuori, e viene usata la statistica F, di cui abbiamo le stelline come al solito.
La seconda parte del comando e' anova per il test non parametrici. Quesot confronta il modello con la spline `s(PhD, 5)` con quella in cui la variabile entra in maniera lineare (senza spline).
Quindi confronta:
y = beta0 + beta1 PhD + ...
con quella con
y = beta0 + beta1 s(PhD, 5) + ...
Non ne possiamo togliere nessuna comunque perche' sono tutte significative.
Vediamo graficamente:
```
par(mfrow=c(1,4))
plot(m.gam.sp)
```

In ogni caso e' meglio usare la smoothing spline perche' fa da sola la cross-validation (cv). Il tutto si puo' fare anche con un modello di regressione logistica, dove devo solo fissare un livello per dire se il livello e' alto o basso.

Quindi devo procedere nella seguente maniera:
- Prima eseguo una analisi grafica
- Se noto non linearita' allora provo con una spline
- Se invece ho andamenti lineari o parabolici nel grafico e' meglio usare il modello lineare.
