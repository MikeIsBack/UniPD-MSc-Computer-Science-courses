Carichiamo i dati dell'altra volta:
```
library(ISLR)
data(Hitters)
hitters <- na.omit(Hitters)
```

Useremo la funzione `glmnet`, carichiamola:
```
library(glmnet)
```
Assegniamo:
```
y <- hitters$Salary
```
Nota: Per usare la funzione `glmnet` dobbiamo passargli gia' le variabili dummy.
Ora digitiamo:
```
X <- model.matrix(Salary ~ ., data=hitters)[,-1]
```

Usiamo ridge ora, e calcoliamolo usando `glmnet`:
```
m.ridge <- glmnet(X, y, alpha=0)
```
Possiamo vederne i nomi ora e la lunghezza:
```
names(m.ridge)
length(m.ridge$lambda)
dim(m.ridge$beta)
```

Il plot del modello ci da una idea di come si comporta la stima dei coefficenti:
```
plot(m.ridge)
```

Questo non e' il grafico migliore per vedere ridge, in quanto ci da il grafico dei coefficenti rispetto alla norma 1, per cui ci conviene cambiare l'asse delle ascisse. Quindi aggiungeremo `xvar='lambda'`:
```
plot(m.ridge, xvar='lambda')
```

Ora facciamo un'analisi, eseguiamo cv per scegliere lambda:
```
set.seed(2906)
cv.ridge <- cv.glmnet(X, y, alpha=0)
```

Vediamo i nomi ottenuti:
```
names(cv.ridge)
```
Eseguiamone il plot:
```
plot(cv.ridge)
```

Il lambda minimo del rigde sara' quindi:
```
lambda.min.ridge <- cv.ridge$lambda.min
```
Su una scala normale.
Quale sarebbe il valore minimo di cross validation?
```
cv.ridge$cvm[cv.ridge$lambda == lambda.min.ridge]
```

Che ci da 115483.8. Estraiamo il massimo:
```
max(cv.ridge$dev.ratio)
names(cv.ridge)
```

Calcoliamo il ridge minimo:
```
m.ridge.min <- glmnet(X, y, alpha=0, lambda=lambda.min.ridge)
```

Vediamone il coefficenti:
```
> coef(m.ridge.min)
20 x 1 sparse Matrix of class "dgCMatrix"
                       s0
(Intercept)  7.564046e+01
AtBat       -6.313804e-01
Hits         2.642432e+00
HmRun       -1.366176e+00
Runs         1.056712e+00
RBI          7.338968e-01
Walks        3.272252e+00
Years       -8.612151e+00
CAtBat       8.024796e-04
CHits        1.303623e-01
CHmRun       6.757304e-01
CRuns        2.766693e-01
CRBI         2.544577e-01
CWalks      -2.564581e-01
LeagueN      5.229215e+01
DivisionW   -1.224408e+02
PutOuts      2.622233e-01
Assists      1.621650e-01
Errors      -3.645430e+00
NewLeagueN  -1.700655e+01
```

Costruiamo ora un modello lineare:
```
m <- lm(Salary ~ ., data=hitters)
```
E confrontiamone i coefficenti. Non tutti saranno significativi, ora vedremo:
```
> cbind(coef(m), coef(m.ridge.min))
20 x 2 sparse Matrix of class "dgCMatrix"
                                    s0
(Intercept)  163.1035878  7.564046e+01
AtBat         -1.9798729 -6.313804e-01
Hits           7.5007675  2.642432e+00
HmRun          4.3308829 -1.366176e+00
Runs          -2.3762100  1.056712e+00
RBI           -1.0449620  7.338968e-01
Walks          6.2312863  3.272252e+00
Years         -3.4890543 -8.612151e+00
CAtBat        -0.1713405  8.024796e-04
CHits          0.1339910  1.303623e-01
CHmRun        -0.1728611  6.757304e-01
CRuns          1.4543049  2.766693e-01
CRBI           0.8077088  2.544577e-01
CWalks        -0.8115709 -2.564581e-01
LeagueN       62.5994230  5.229215e+01
DivisionW   -116.8492456 -1.224408e+02
PutOuts        0.2818925  2.622233e-01
Assists        0.3710692  1.621650e-01
Errors        -3.3607605 -3.645430e+00
NewLeagueN   -24.7623251 -1.700655e+01
```
Le prime due intercette non sono da confrontare in quanto parte di due modelli diversi.

Per vedere l'efficacia dello sciacciamento dovremmo calcolare:
- MSE di ridge
- MSE di CV di lm()

Ma il secondo R non riesce a stimarlo.
Vediamo il grafico:
```
plot(m.ridge, xvar='lambda')
```
Aggiungiamo una retta verticale:
```
abline(v=log(lambda.min.ridge), lty=2, lwd=2)
```
Questo aggiunge una linea tratteggiata che rappresenta il minimo ridge. Questa e' appena dopo la stimadei minimi quadrati.

L'altra regolarizza di piu' ma perde in scarto quadratico medio:
```
lambda.ridge.1se <- cv.ridge$lambda.1se
```
Questo ci da 24363.791
Plottiamolo nel grafico:
```
abline(v=log(lambda.ridge.1se), lty=2, lwd=2)
```
Questo ci rappresenta il lambda ad un standard error di distanza.

Alla fine il modello migliore dipende da quanto vogliamo regolarizzare.
Cambiamo grafico, e diamo:
```
min(cv.ridge$cvm)
cv.ridge$cvm[cv.ridge$lambda == lambda.ridge.1se]
plot(m.ridge$lambda, m.ridge$dev.ratio, type='l')
abline(v=lambda.min.ridge, lty=2, lwd=2)
```

Passiamo a vedere cosa succede con il Lasso:
```
m.lasso <- glmnet(X, y, alpha=1)
m.lasso
plot(m.lasso)
plot(m.lasso, xvar='lambda')

set.seed(2906)
cv.lasso <- cv.glmnet(X, y, alpha=1)
cv.lasso$lambda.min
cv.lasso$lambda.1se

abline(v=log(cv.lasso$lambda.min), lty=2, lwd=2)
abline(v=log(cv.lasso$lambda.1se), lty=2, lwd=2)
```

Proviamo a fare dei conti leggermente diversi con il lasso:
```
set.seed(2906)
cv.lasso <- cv.glmnet(X, y, alpha=1, type.measure='deviance')
min(m.lasso$cvm)
min(cv.ridge$cvm)

m.lasso.min <- glmnet(X, y, alpha=1, lambda=cv.lasso$lambda.min)
```

Ora confrontiamo il coefficenti del ridge e del lasso con lambda minimo
```
> cbind(coef(m), coef(m.ridge.min), coef(m.lasso.min))
20 x 3 sparse Matrix of class "dgCMatrix"
                                    s0           s0
(Intercept)  163.1035878  7.564046e+01  117.4722225
AtBat         -1.9798729 -6.313804e-01   -1.4953199
Hits           7.5007675  2.642432e+00    5.5496308
HmRun          4.3308829 -1.366176e+00    .        
Runs          -2.3762100  1.056712e+00    .        
RBI           -1.0449620  7.338968e-01    .        
Walks          6.2312863  3.272252e+00    4.6299630
Years         -3.4890543 -8.612151e+00   -8.9600862
CAtBat        -0.1713405  8.024796e-04    .        
CHits          0.1339910  1.303623e-01    .        
CHmRun        -0.1728611  6.757304e-01    0.5292168
CRuns          1.4543049  2.766693e-01    0.6516310
CRBI           0.8077088  2.544577e-01    0.3734921
CWalks        -0.8115709 -2.564581e-01   -0.5102188
LeagueN       62.5994230  5.229215e+01   31.9723644
DivisionW   -116.8492456 -1.224408e+02 -118.9383665
PutOuts        0.2818925  2.622233e-01    0.2713797
Assists        0.3710692  1.621650e-01    0.1617246
Errors        -3.3607605 -3.645430e+00   -1.9131382
NewLeagueN   -24.7623251 -1.700655e+01    .        
```

Il puntino non centra nulla con la significativita' del modello lineare. Qui il lasso elimina 6 variabili. Non e' detta che questa selezione sia la stessa con i metodi visti l'altra volta (aic, bic)...questo modo e' molto efficente.
Creiamo il modello lineare multivariato con le variabili selezionate:
```
m.selezione <- lm(Salary ~ AtBat+Hits+Walks+Years+CHmRun+CRuns+CRBI+CWalks+League+Division+PutOuts+Assists+Errors, data=hitters)
```
Nota: va solo messo il nome senza la lettera finale nelle variabili dummy.
Vediamo il summary:
```
> summary(m.selezione)

Call:
lm(formula = Salary ~ AtBat + Hits + Walks + Years + CHmRun + 
    CRuns + CRBI + CWalks + League + Division + PutOuts + Assists + 
    Errors, data = hitters)

Residuals:
    Min      1Q  Median      3Q     Max 
-940.10 -174.20  -25.94  127.05 1890.12 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  187.55948   88.57177   2.118 0.035200 *  
AtBat         -2.30798    0.56236  -4.104  5.5e-05 ***
Hits           7.34602    1.71760   4.277  2.7e-05 ***
Walks          6.08610    1.57008   3.876 0.000136 ***
Years        -13.60502   10.38333  -1.310 0.191310    
CHmRun         0.83633    0.84709   0.987 0.324457    
CRuns          0.90924    0.27662   3.287 0.001159 ** 
CRBI           0.35734    0.36252   0.986 0.325229    
CWalks        -0.83918    0.27207  -3.084 0.002270 ** 
LeagueN       36.68460   40.73468   0.901 0.368685    
DivisionW   -119.67399   39.32485  -3.043 0.002591 ** 
PutOuts        0.29296    0.07632   3.839 0.000157 ***
Assists        0.31483    0.20460   1.539 0.125142    
Errors        -3.23219    4.29443  -0.753 0.452373    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 314 on 249 degrees of freedom
Multiple R-squared:  0.5396,	Adjusted R-squared:  0.5156 
F-statistic: 22.45 on 13 and 249 DF,  p-value: < 2.2e-16
```
Nota: gli standard error stimati sono piu' grandi di quelli che dovrebbero essere, e non si sa ancora come gestirli quando si usa il lasso. Se calcolassi l'intervallo di confidenza avrei degli errori, perche' sarebbero troppo piccoli rispetto a quello che dovrebbero essere.
Anche le stelline in questo caso son da prendere con le pinze.

Possiamo prevedere i valori al di fuori nel training set con il comando `predict`.
Diamo il comando:
```
previsione.ridge <- predict(m.ridge.min, newx=X)

previsione.lasso <- predict(m.lasso.min, newx=X)

plot(hitters$Salary, previsione.ridge, pch=16)
abline(0,1)
```

Sono costruiti con media nulla e varianza costante. Con ridge si parte con penalizzazione e quindi grafico res, e non si interpreta nello stesso modo.
Digitiamo:
```
points(hitters$Salary, previsione.lasso, pch=16, col='red')
```
Qui abbiamo guadagnato qualcosa ma non e' un grande quadagno alla fin fine.
