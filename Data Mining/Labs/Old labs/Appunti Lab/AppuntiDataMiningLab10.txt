Riprendiamo i dati dell'altra volta
```
library(pls)
data(gasoline)
y <- gasoline$octane
X <- gasoline$NIR
dim(X)
```

Con 5 componenti principali, che erano state costruite con il comando:
```
m.pcr <- pcr(y ~ X, scale=TRUE, ncomp=5)
```

Prende 5 componenti principali che son quelle che avevamo scelto la scorsa volta.
Avevamo visto anche i grafici per vedere come variavano i coefficenti delle componenti principali (ovvero che la prima identificava le X come piu' importanti, Y le altre e avanti cosi', finche' non rimaneva piu' niente da evidenziare)
I comando per visualizzare il grafico e' `coefplot(m.pcr)

Vediamo lo `scoreplot(m.pcr, comps=1:5)` che ci permetteva di notare raggruppamenti particolari, o outlier. L'importante e' che piu' avanti con le componenti principali queste spariscano.
Vediamo lo spazio dei nomi di `m.pcr`.
```
> names(m.pcr)
 [1] "coefficients"  "scores"        "loadings"      "Yloadings"    
 [5] "projection"    "Xmeans"        "Ymeans"        "fitted.values"
 [9] "residuals"     "Xvar"          "Xtotvar"       "fit.time"     
[13] "ncomp"         "method"        "scale"         "call"         
[17] "terms"         "model"        
```

Facciamo attenzione che `residuals` non centrano niente con i nostri residui che conosciamo.

Carichiamo ridge e lasso:
```
library(glmnet)
```

Eseguiamo le seguenti operazioni:
```
set.seed(222) #settiamo il seed come nelle dispense
cv.ridge <- cv.glmnet(X, y, alpha=0, lambda.min=1e-4) #alpha=0 per specificare ridge
```

Prendiamo il lambda minimo e vediamo quant'e':
```
> cv.ridge$lambda.min
[1] 0.3476072
```
Quello con la standard error e':
```
> cv.ridge$lambda.1se
[1] 1.540116
```

Ora eseguiamo un confronto "veloce" tra ridge e lasso, che non e' il quadro completo:
```
m.ridge.min <- glmnet(X, y, alpha=0, lambda=cv.ridge$lambda.min)
```
Ora vediamo quanto vale il minimo dell'errore quadratico medio:
```
> min(cv.ridge$cvm)
[1] 0.04543276
```
Questo e' il minimo errore quadratico medio. A questo punto eseguiamo il lasso, vediamo cosa fa e confrontiamo l'errore quadratico medio.

Quindi per il lasso eseguiamo:
```
set.seed(222) #cosi' abbiamo un confronto uguale sugli stessi dati
cv.lasso <- cv.glmnet(X, y, alpha=1, lambda.min=1e-4) #alpha=1 per lasso
```

Otteniamo che il lambda minimo e':
```
> cv.lasso$lambda.min
[1] 0.01086521
```

```
> m.lasso.min <- glmnet(X, y, alpha=1, lambda=cv.lasso$lambda.min)
> min(cv.lasso$cvm)
[1] 0.04645222
```
Ora confrontiamolo con quello che abbiamo ottenuto.
```
id.zero <- which (coef(m.lasso.min)==0)
length(id.zero)
```

Il risultato ci dice che sono stati messi a zero 381 coefficenti.
Vedendo le dimensioni otteniamo che `dim(X)` = 401, quindi sono 400 variabili - 381 = 19 variabili in meno.
In questo caso eliminerei il ridge in favore del lasso, perche' gli errori quadratici medi son simili ma il lasso ha molte meno variabili.

Usiamo la funzione `MSEP` per calcolare l'errore quadratico medio della previsione della regressione delle componenti principali in due casi:
- Quando teniamo l'intercetta
- E quando usiamo 5 componenti

```
> MSEP(m.pcr, ncom=5)
(Intercept)      5 comps  
    2.30212      0.04187  
```

Ricalcoliamo con 20 componenti principali:
```
set.seed(222)
cv.pcr <- pcr(y ~ X, scale=TRUE, ncomp=20, validation='CV')
```

Ora rivediamo il MSEP:
```
> MSEP(cv.pcr, ncomp=5)
       (Intercept)  5 comps
CV           2.381  0.05555
adjCV        2.381  0.05465
```

Anche qui sceglierei il lasso.
Poi dipende anche dalla persona a cui consegniamo i dati, se preferisce avere 19 variabili o 5, in base anche a cosa deve fare.


